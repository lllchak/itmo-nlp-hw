{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb4220a6-6238-4bfb-977b-b290856b4de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-04 18:23:39--  https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "connected. to github.com (github.com)|140.82.121.4|:443... \n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250304T152339Z&X-Amz-Expires=300&X-Amz-Signature=2387c74b60fe74410418ce36ac0bc1d5e5c6f4237d441bff4bfc78c0b59d9dd0&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-03-04 18:23:39--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250304T152339Z&X-Amz-Expires=300&X-Amz-Signature=2387c74b60fe74410418ce36ac0bc1d5e5c6f4237d441bff4bfc78c0b59d9dd0&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 527373240 (503M) [application/octet-stream]\n",
      "Saving to: ‘lenta-ru-news.csv.gz.1’\n",
      "\n",
      "lenta-ru-news.csv.g 100%[===================>] 502,94M  17,4MB/s    in 29s     \n",
      "\n",
      "2025-03-04 18:24:09 (17,2 MB/s) - ‘lenta-ru-news.csv.gz.1’ saved [527373240/527373240]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67cdeca3-3bc9-47f8-98ce-1fb7028e57e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: nltk in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (1.6.0)\n",
      "Requirement already satisfied: corus in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (0.10.0)\n",
      "Requirement already satisfied: legacy-cgi in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (2.6.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (from pandas) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: click in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lulchak-pavel/venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas nltk scikit-learn corus legacy-cgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e4a9052-25be-48f8-99df-85947b981009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/lulchak-\n",
      "[nltk_data]     pavel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lulchak-\n",
      "[nltk_data]     pavel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/lulchak-\n",
      "[nltk_data]     pavel/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from corus import load_lenta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = 'lenta-ru-news.csv.gz'\n",
    "\n",
    "SIZE_LIMIT = 150_000\n",
    "TOPIC_SIZE_THRESHOLD = 10\n",
    "\n",
    "CORPUS = [\n",
    "    {'text': row.title + '. ' + row.text, 'topic': row.topic}\n",
    "    for row in load_lenta(DATA_PATH)\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(CORPUS)\n",
    "df = df.sample(n=SIZE_LIMIT, random_state=123).reset_index(drop=True)\n",
    "\n",
    "assert len(df) == SIZE_LIMIT\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('russian'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01e98969-ce67-461f-9d96-aa668fdbdcec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>туляку дали полтора года тюрьмы экстремизм вко...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>microsoft google готовы уладить дело кайфу аме...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>киеве митингующие ворвались здание минюста пон...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  topic\n",
       "0  туляку дали полтора года тюрьмы экстремизм вко...      7\n",
       "1  microsoft google готовы уладить дело кайфу аме...      7\n",
       "2  киеве митингующие ворвались здание минюста пон...      4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text: str):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    \n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in STOP_WORDS]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "topic_counts = df['topic'].value_counts()\n",
    "other_topics = topic_counts[topic_counts < TOPIC_SIZE_THRESHOLD].index\n",
    "\n",
    "df['topic'] = df['topic'].apply(lambda x: 'Other' if x in other_topics else x)\n",
    "df['topic'] = df['topic'].astype('category').cat.codes\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "205cd222-0798-40c2-9c15-a2f62917b800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "Библиотека    9\n",
       "ЧМ-2014       1\n",
       "Оружие        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_counts[topic_counts < TOPIC_SIZE_THRESHOLD]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6663f630-c0e7-4410-9fbe-cfc38bbec942",
   "metadata": {},
   "source": [
    "**Пайплайн предобработки**:\n",
    "\n",
    "1. Приводим текст к единому регистру + удаляем небуквенные символы\n",
    "2. Лемматизируем исходный текст, так как хотим учитывать контекст при обработке\n",
    "3. Топики привел целочисленным значениям и объеденил редкие топики в единый топик Other (всего 11 записей, кажется ок)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db6c32d8-c421-4b19-bec6-d4c279084637",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['topic']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4c16929-99c1-4897-aa18-c1d456350a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.00      0.00      0.00        46\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.01      0.01      0.01       298\n",
      "           4       0.07      0.07      0.07      2153\n",
      "           5       0.02      0.02      0.02       901\n",
      "           6       0.03      0.03      0.03      1109\n",
      "           7       0.06      0.06      0.06      1803\n",
      "           8       0.00      0.00      0.00        27\n",
      "           9       0.00      0.00      0.00        15\n",
      "          10       0.08      0.08      0.08      2180\n",
      "          11       0.00      0.00      0.00         6\n",
      "          12       0.18      0.18      0.18      5587\n",
      "          13       0.08      0.08      0.08      2147\n",
      "          14       0.00      0.00      0.00       259\n",
      "          15       0.21      0.21      0.21      6531\n",
      "          16       0.02      0.02      0.02       784\n",
      "          17       0.08      0.09      0.08      2610\n",
      "          18       0.01      0.01      0.01       319\n",
      "          19       0.11      0.11      0.11      3215\n",
      "\n",
      "    accuracy                           0.12     30000\n",
      "   macro avg       0.05      0.05      0.05     30000\n",
      "weighted avg       0.12      0.12      0.12     30000\n",
      "\n",
      "F1-score: 0.1224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy='stratified', random_state=42)\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'F1-score: {f1_score(y_test, y_pred, average=\"weighted\"):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f106d-fad9-473a-aa9d-8c21f3fd4f5f",
   "metadata": {},
   "source": [
    "Далеко не то качество, которое хочется получить, попробуем обучить лог. регрессию на векторизованных текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beab7154-d85d-4e1d-a810-cb444df871c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score with CountVectorizer: 0.8152\n",
      "F1-score with TfidfVectorizer: 0.8044\n"
     ]
    }
   ],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def fit_predict_logreg(\n",
    "    vectorizer: Union[CountVectorizer, TfidfVectorizer],\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    ") -> List[int]:\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    model = LogisticRegression(max_iter=5_000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model.predict(X_test)\n",
    "   \n",
    "f1_count = f1_score(y_test, fit_predict_logreg(CountVectorizer(), X_train.copy(), y_train.copy(), X_test.copy()), average='weighted')\n",
    "print(f'F1-score with CountVectorizer: {f1_count:.4f}')\n",
    "\n",
    "f1_tfidf = f1_score(y_test, fit_predict_logreg(TfidfVectorizer(), X_train.copy(), y_train.copy(), X_test.copy()), average='weighted')\n",
    "print(f'F1-score with TfidfVectorizer: {f1_tfidf:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d30ac26-b5ac-4c36-954f-64f1a6fa2543",
   "metadata": {},
   "source": [
    "Взвешенный F1 скор выглядит приемлемо, попробуем подобрать параметры на кросс-валидации для TF-IDF эмбеддера. Пусть с CountVectorizer показывает больший скор на тесте все таки остановлюсь на TF-IDF так как для задачи важно понимать редко встречающиеся аббривеатуры и другие специфичные для новостей тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d95a8484-295e-4b85-91fd-7d0277b02eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV 3/3; 3/12] START classifier__C=0.1, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 3/12] END classifier__C=0.1, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.714 total time=  18.7s\n",
      "[CV 3/3; 6/12] START classifier__C=0.5, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 6/12] END classifier__C=0.5, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2);, score=0.783 total time= 1.0min\n",
      "[CV 2/3; 1/12] START classifier__C=0.1, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 1/12] END classifier__C=0.1, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.708 total time=  18.2s\n",
      "[CV 2/3; 5/12] START classifier__C=0.5, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 5/12] END classifier__C=0.5, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.776 total time=  22.3s\n",
      "[CV 2/3; 7/12] START classifier__C=0.5, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 7/12] END classifier__C=0.5, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.776 total time=  21.3s\n",
      "[CV 2/3; 8/12] START classifier__C=0.5, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 8/12] END classifier__C=0.5, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.777 total time=  52.7s\n",
      "[CV 3/3; 1/12] START classifier__C=0.1, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 1/12] END classifier__C=0.1, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.714 total time=  17.9s\n",
      "[CV 3/3; 5/12] START classifier__C=0.5, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 5/12] END classifier__C=0.5, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.781 total time=  21.9s\n",
      "[CV 3/3; 7/12] START classifier__C=0.5, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 7/12] END classifier__C=0.5, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 1);, score=0.781 total time=  20.0s\n",
      "[CV 2/3; 10/12] START classifier__C=1, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 10/12] END classifier__C=1, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2);, score=0.790 total time=  51.7s\n",
      "[CV 3/3; 4/12] START classifier__C=0.1, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 4/12] END classifier__C=0.1, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.714 total time= 1.1min\n",
      "[CV 3/3; 10/12] START classifier__C=1, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 10/12] END classifier__C=1, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2);, score=0.795 total time=  51.5s\n",
      "[CV 2/3; 2/12] START classifier__C=0.1, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 2/12] END classifier__C=0.1, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 2);, score=0.708 total time= 1.1min\n",
      "[CV 2/3; 9/12] START classifier__C=1, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1)\n",
      "[CV 2/3; 9/12] END classifier__C=1, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.789 total time=  18.6s\n",
      "[CV 2/3; 12/12] START classifier__C=1, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 2/3; 12/12] END classifier__C=1, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.790 total time=  45.0s\n",
      "[CV 1/3; 4/12] START classifier__C=0.1, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 1/3; 4/12] END classifier__C=0.1, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.707 total time= 1.1min\n",
      "[CV 3/3; 9/12] START classifier__C=1, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1)\n",
      "[CV 3/3; 9/12] END classifier__C=1, classifier__solver=saga, vectorizer__min_df=1, vectorizer__ngram_range=(1, 1);, score=0.793 total time=  19.3s\n",
      "[CV 3/3; 12/12] START classifier__C=1, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2)\n",
      "[CV 3/3; 12/12] END classifier__C=1, classifier__solver=saga, vectorizer__min_df=5, vectorizer__ngram_range=(1, 2);, score=0.796 total time=  43.7s\n",
      "Best parameters found: {'classifier__C': 1, 'classifier__solver': 'saga', 'vectorizer__min_df': 5, 'vectorizer__ngram_range': (1, 2)}\n",
      "Optimized F1-score: 0.8001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_features=10_000)),\n",
    "    ('classifier', LogisticRegression(max_iter=5_000, random_state=42))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "    'vectorizer__min_df': [1, 5],\n",
    "    'classifier__C': [0.1, 0.5, 1],\n",
    "    'classifier__solver': ['saga']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, cv=3, verbose=10, scoring=make_scorer(f1_score, average='weighted'))\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "y_pred_best = grid_search.predict(X_test)\n",
    "f1_best = f1_score(y_test, y_pred_best, average='weighted')\n",
    "print(f\"Optimized F1-score: {f1_best:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c84f27df-9cf3-49af-9bc0-fe9d3e4eec9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.00      0.00      0.00        47\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.76      0.26      0.39       298\n",
      "           4       0.83      0.81      0.82      2153\n",
      "           5       0.85      0.77      0.81       901\n",
      "           6       0.66      0.56      0.61      1110\n",
      "           7       0.75      0.68      0.72      1803\n",
      "           8       0.00      0.00      0.00        27\n",
      "           9       0.00      0.00      0.00        14\n",
      "          10       0.86      0.86      0.86      2179\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.78      0.85      0.81      5587\n",
      "          13       0.83      0.84      0.84      2147\n",
      "          14       0.87      0.47      0.61       260\n",
      "          15       0.77      0.84      0.80      6531\n",
      "          16       0.76      0.46      0.57       783\n",
      "          17       0.96      0.95      0.95      2610\n",
      "          18       0.96      0.72      0.82       320\n",
      "          19       0.82      0.86      0.84      3215\n",
      "\n",
      "    accuracy                           0.81     30000\n",
      "   macro avg       0.57      0.50      0.52     30000\n",
      "weighted avg       0.80      0.81      0.80     30000\n",
      "\n",
      "Optimized Weighted F1-score: 0.8023\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "y_pred_val = grid_search.predict(X_val)\n",
    "\n",
    "report = classification_report(y_val, y_pred_val)\n",
    "\n",
    "f1_test = f1_score(y_val, y_pred_val, average='weighted')\n",
    "\n",
    "print(report)\n",
    "print(f\"Optimized Weighted F1-score: {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d0c34-de0b-43d1-8ff2-3df1ed521399",
   "metadata": {},
   "source": [
    "Видно, что алгоритм хорошо выучил содержательные категории, но на малых данных видно проблемы в качестве, кажется, обучение на больших данных даст лучший результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad3b469-601d-4c2b-b03f-bf8ee9d17ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
